{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器学习实战 Machine Learning in Action\n",
    "#### 4. navie bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "class myself_OneHotEncoder(object):\n",
    "    def get_Dict(self, labels): #data is a list\n",
    "        label_dict = {}\n",
    "        #create a label dictionary\n",
    "        for i,lable in enumerate(set(labels)):\n",
    "            label_dict[lable] = i\n",
    "        \n",
    "        #create label exchange keys,values\n",
    "        label_dict_reverse = dict((v,k) for k,v in label_dict.items())\n",
    "        \n",
    "        return label_dict,label_dict_reverse\n",
    "    \n",
    "    def get_OneHotEncoder(self,labels):\n",
    "        # get labels to integer\n",
    "        label_dict,_ = self.get_Dict(labels)\n",
    "        integer_encoded = []\n",
    "        for label in labels:\n",
    "            integer_encoded.append(label_dict[label])\n",
    "        \n",
    "        # get integer labels to OneHot labels\n",
    "        integer_encoded = np.array(integer_encoded)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "        return onehot_encoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "class bayes:\n",
    "    \n",
    "    def createVocabList(self,dataset):\n",
    "        vocabSet = set([])\n",
    "        for document in dataset:\n",
    "            vocabSet = vocabSet | set(document)\n",
    "        return list(vocabSet)\n",
    "    \n",
    "    def setofWord2Vec(self,vocabList,inputSet):\n",
    "        returnVec = [0]*len(vocabList) #a list \n",
    "        for word in inputSet:\n",
    "            if word in vocabList:\n",
    "                returnVec[vocabList.index(word)] = 1\n",
    "            else :\n",
    "                print('the word: %s is not in my Vocabulary! ' %word)\n",
    "        return returnVec\n",
    "    \n",
    "    #navie bayes classifier training\n",
    "    def trainNB0(self,dataset, labels):   #labels type with onehot\n",
    "        self.labels = labels\n",
    "        self.vocabSetlist = self.createVocabList(dataset)\n",
    "        train_Matrix = []\n",
    "        for w in dataset: \n",
    "            train_Matrix.append(self.setofWord2Vec(self.vocabSetlist,w))\n",
    "        print('train_Matrix',train_Matrix)\n",
    "        numTrainDocs = len(train_Matrix)  #num of samples\n",
    "        numWords = len(train_Matrix[0])  #num of features\n",
    "        #self.pAbusive = sum(labels)/float(numTrainDocs) #样本为1的比例\n",
    "        #mulity classifier\n",
    "        '''\n",
    "        #public onehot_encoded\n",
    "        OneHot = myself_OneHotEncoder()\n",
    "        onehot_encoded = OneHot.get_OneHotEncoder(labels)\n",
    "        print('onehot_encoded',onehot_encoded)\n",
    "        n_labels = onehot_encoded.shape[1]\n",
    "        '''\n",
    "        n_labels = labels.shape[1]\n",
    "        p_classes = {}\n",
    "        for i in range(n_labels):\n",
    "            p_classes[i] = np.sum(labels[:,i])\n",
    "              \n",
    "        p_Num = {}\n",
    "        p_Denom = {}\n",
    "        for label in range(n_labels):\n",
    "            for i in range(numTrainDocs):\n",
    "                if labels[i][label] == 1:\n",
    "                    #print('np.where(onehot_encoded[i]==1)[0]',np.where(onehot_encoded[i]==1)[0])\n",
    "                    print(i)\n",
    "                    if label not in p_Num.keys():\n",
    "                        p_Num[label] = np.zeros(numWords)\n",
    "                        p_Denom[label] = 0.0\n",
    "                    p_Num[label] += train_Matrix[i]\n",
    "                    p_Denom[label] += sum(train_Matrix[i])\n",
    "                    #print(\"p_Num:{} '\\n' p_Denom:{}\".format(p_Num,p_Denom))\n",
    "        \n",
    "        p_Vec = {}\n",
    "        for label in range(n_labels):\n",
    "            if label not in p_Vec:\n",
    "                p_Vec[label] = 0.0\n",
    "            #log\n",
    "            p_Vec[label] = np.log(p_Num[label]/p_Denom[label] + 1e-9)\n",
    "            #p_Vec[label] = p_Num[label]/p_Denom[label] \n",
    "        return p_Vec,p_classes     \n",
    "    \n",
    "    \n",
    "    #\n",
    "    def classifyNB(self,vec2Classify, p_Vec, p_Classes):\n",
    "        # 元素相乘 如果log是相加\n",
    "        '''\n",
    "        p_1 = sum(vec2Classify * p_1_Vec)  + np.log(pClass1)\n",
    "        p_0 = sum(vec2Classify * p_0_Vec) + np.log(1.0 - pClass1)\n",
    "        if p_1 > p_0:\n",
    "            return 1\n",
    "        else :\n",
    "            return 0\n",
    "        '''\n",
    "        p = {}\n",
    "        for label in p_Classes.keys():\n",
    "            p[label] = sum(vec2Classify * p_Vec[label])  + np.log(p_Classes[label])\n",
    "        p_max = Counter(p).most_common(2) #dict sorted by values. If you want get mulity result , you can set most_common(n)\n",
    "        return p_max\n",
    "        \n",
    "    def testingNB(self,test_data,p_Vec, p_Classes):\n",
    "        #myVocabList = createVocabList(test_data)\n",
    "        test_Matrix =[]\n",
    "        preds = []\n",
    "        for testd in test_data:\n",
    "            test_Matrix = self.setofWord2Vec(self.vocabSetlist,testd)\n",
    "            pred = self.classifyNB(test_Matrix,p_Vec, p_Classes)\n",
    "            preds.append(pred)\n",
    "            \n",
    "        return preds\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def words_split(dataset):\n",
    "    data_split = []\n",
    "    regEx = re.compile('\\\\W*')\n",
    "    for data in dataset:\n",
    "        data_temp = [d.lower() for d in regEx.split(data) if len(d)>0]\n",
    "        data_split.append(data_temp)\n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['my dog has flea, problems help please',\n",
    "        'maybe not take him to dog park stupid',\n",
    "        'my dalmation is so cute I love him',\n",
    "        'stop posting stupid worthless garbage',\n",
    "         ' mr licks ate my steak how to stop him',\n",
    "        'quit buying worthless dog food stupid',\n",
    "        'this noing i just look at it',\n",
    "        'everything is ok']\n",
    "words = words_split(words)\n",
    "#classVec = [0,1,0,1,0,1,2,2]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = bayes()\n",
    "myVocablist = bayes.createVocabList(words)\n",
    "#myword2Vec = bayes.setofWord2Vec(myVocablist,words[0])\n",
    "myword2Vec = []\n",
    "for word_1 in words :\n",
    "    myword2Vec.append(bayes.setofWord2Vec(myVocablist,word_1))\n",
    "len(myVocablist)#,myword2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVocablist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classVec = np.array([[1,0,1],\n",
    " [0, 1, 0],\n",
    " [1, 0, 0],\n",
    " [0, 1, 1],\n",
    " [1, 0, 0],\n",
    " [0, 1, 0],\n",
    " [1, 0, 1],\n",
    " [0, 0 ,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes = bayes()\n",
    "p_Vec, p_classes = bayes.trainNB0(words,classVec)\n",
    "p_Vec, p_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bayes.classifyNB(myword2Vec[4], p_Vec, p_classes)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ['love my dalmation','stupid dog']\n",
    "test_data = words_split(test_data)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bayes.testingNB(test_data,p_Vec,p_classes)\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
